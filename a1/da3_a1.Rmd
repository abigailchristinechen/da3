---
title: "DA3-A1"
author: "Abigail Chen"
prettydoc::html_pretty:
    theme: cayman
    highlight: github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Clean environment
rm(list=ls())

# loading the libraries
# Load packages
library(tidyverse)
library(caret) 
library(ggplot2)
library(forecast)
library(flexmix)

#preventing scientific notations
options(scipen=999)

```


# Introduction

For this assignment the goal is to use the parameters given in the dataset, [cps-earnings dataset](https://osf.io/g8p9j/), to build various kinds of predictive models using linear regression for hourly wage for **Editors**. The data is taken from Current Population Survey (CPS), which is a survey for the employment and labor markets. After data cleaning, parameters will be chosen with its relation to the parameter of interest. These will be tested in terms of (a) RMSE in the full sample, (b) BIC in the full sample and (c) cross-validated RMSE. Finally, then discussing the relationships between the different models and its performance.

# Data Cleaning, EDA and Transformation (see details in Appendix)

```{r, include=FALSE}
## Loading the data from the web 

df = read.csv( 'https://osf.io/4ay9x/download')

## Check the data table
glimpse(df)
```

## Selecting the occupation 
  
  Selected: Editors; 2830

```{r }
## Selecting the Editors with code : 2830
df = df[df$occ2012  == '2830' ,]

## Check the data table
glimpse(df)
```

## Data Cleaning 

## Check the structure of the data
```{r }
str(df)
```

## Parameters Selection
Initial checking of various parameters to try based on intuition, to see which parameters can affect the wage.

```{r }
df = df[, c('weight','earnwke', 'uhours', 'race', 'age', 'sex', 'marital','grade92', 'state','class','prcitshp')]
```

## Creating variable of interest: Hourly Wage 

```{r }
df$HourlyWage = round(df$earnwke/df$uhours)
```


## Parameters Selection:
Analyzing the different parameters initially chosen

## Weight

```{r }
weight_summary <- summary(df$weight)
weight_summary
## Min. 1st Qu.  Median    Mean   3rd Qu.    Max. 
## 222.1  1054.6  2769.5  2375.1  3427.1  7469.4 
```


```{r }
## Check weight graph
 ggplot(df, aes(x=weight, y=HourlyWage)) + geom_point()

```

Visualization shows a random spread.


```{r }
## Tried using Pearson Coefficient to see best fit for the various parameters
## https://statistics.laerd.com/statistical-guides/pearson-correlation-coefficient-statistical-guide.php
## https://libguides.library.kent.edu/spss/pearsoncorr
## used the guideline here to interpret the Pearson's correlation coefficient
## okay to try, since weight is continuous parameter
## tried the code chunk from https://www.reneshbedre.com/blog/correlation-analysis-r.html
cor(df$HourlyWage, df$weight, method = 'pearson')
## -0.1220714
```

Here, we can see using the pearson correlation that the weight has very low negative correlation to the hourly wage. 
So, it will not be considered as a predictor parameter. 

## Race 

```{r }
unique(df$race)

```

Converting the race variable to categorical variable.

```{r }

df$race = as.factor(df$race)

```



```{r }

race_box <- ggplot(df, aes(x=race, y=HourlyWage)) + 
  geom_boxplot()
race_box
```
In the visualization, the box plot clearly show there is difference in hourly wage for the different races.


```{r }
## https://www.scribbr.com/statistics/anova-in-r/
## since category
Anova_Race <- aov(HourlyWage ~ race, data = df)

summary(Anova_Race)

## Df Sum Sq Mean Sq F value Pr(>F)  
## race          5   3252   650.3   1.899 0.0974 .
## Residuals   157  53761   342.4                 
## ---
## Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

```

The Anova test confirms that there is difference of hourly wage on the basis of race. However, it is not very significant as the P value is greater than the usual threshold of 0.05. Nonetheless, race will be used as a predictor cause there is relation between hourly wage and race.



## Age

```{r }
age_summary <- summary(df$age)
age_summary
## Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   16.0    29.0    40.0    40.2    50.0    64.0 
```


```{r }
age_plot <- ggplot(df, aes(x=age, y=HourlyWage)) + geom_point()
age_plot
```

Looking at the age_plot, we can see that maybe there is a minor trend of increase in hourly wage with increase in age


```{r }
cor(df$HourlyWage, df$age, method = 'pearson')

## [1] 0.3285981
```

Looking at the correlation test, we can see that there a light positive correlation, so we can keep this parameter.

## Sex
```{r }
unique(df$sex)
```

Converting the race variable to categorical variable.
```{r }
df$sex = as.factor(df$sex)
```


```{r }
ggplot(df, aes(x=sex, y=HourlyWage)) + 
  geom_boxplot()
```

There eems to be difference in pay based on the sex

```{r }

Anova_Sex <- aov(HourlyWage ~ sex, data = df)
summary(Anova_Sex)

#3 #Df Sum Sq Mean Sq F value Pr(>F)  
## sex           1   1846  1846.0   5.387 0.0215 *
## Residuals   161  55167   342.6                 
## ---
##Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
```
For Sex, the Anova test confirms that there is a significant difference pay two sexes as p value is less than the usual threshold of 0.05. So sex parameter will be us

## Marital

```{r }
unique(df$marital)
```

Converting the marital variable to categorical variable.
```{r }
df$marital = as.factor(df$marital)
```


```{r }

marital_box <- ggplot(df, aes(x = marital, y = HourlyWage)) + 
  geom_boxplot()

marital_box 
```

```{r }
Anova_Marital<- aov(HourlyWage ~ marital, data = df)

summary(Anova_Marital)

##              Df Sum Sq Mean Sq F value Pr(>F)
## marital       5   1275   254.9   0.718  0.611
## Residuals   157  55738   355.0    

```

The Marital parameter has a large p-value so it will not be used as a predictor.

## Grade

```{r }
unique(df$grade92)
```

Converting the grade variable to categorical variable.
```{r }
df$grade92 = as.factor(df$grade92)

```


```{r }
grade_box <- ggplot(df, aes(x = grade92, y = HourlyWage)) + 
  geom_boxplot()
grade_box
```

In the grade_box plot, we can see the it seems like there's a good relationship. 

```{r }
Anova_grade92<- aov(HourlyWage ~ grade92, data = df)

summary(Anova_grade92)

## Df Sum Sq Mean Sq F value Pr(>F)  
## grade92       9   7075   786.1   2.408 0.0139 *
## Residuals   153  49938   326.4                 
## Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
```
The ANOVA test shows that hourly wage and grade92 have a significant relation. So grade92 parameter will be used as a predictor.

## Class

```{r }
unique(df$class)
## [1] "Private, For Profit"  "Private, Nonprofit"   "Government - Local"   "Government - Federal"
## [5] "Government - State"  
```

Converting the class variable to categorical variable.
```{r }
df$class = as.factor(df$class)
```


```{r }
class_box <- ggplot(df, aes(x = class, y = HourlyWage)) + 
  geom_boxplot()
class_box
```
The class_box plot showed that the there seems to be a relationship with this parameter

```{r }
Anova_class <- aov(HourlyWage ~ class, data = df)

summary(Anova_class)

## Df Sum Sq Mean Sq F value Pr(>F)  
## class         4   2952   738.0   2.157 0.0763 .
## Residuals   158  54061   342.2                 
## ---
## ignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
```
The Anova test above shows that there is relationship between the class parameter, and will be used a predictor.

## Prcitshp

```{r }
unique(df$prcitshp)
##Native, Born Abroad Of US Parent(s)"    "Native, Born In US"                    
##[3] "Foreign Born, Not a US Citizen"         "Foreign Born, US Cit By Naturalization"
```

```{r }
df$prcitshp = as.factor(df$prcitshp)
```


```{r }

ggplot(df, aes(x = prcitshp, y = HourlyWage)) + 
  geom_boxplot()

```


```{r }

Anova_prcitshp <- aov(HourlyWage ~ prcitshp, data = df)

summary(Anova_prcitshp)

```
The prcitshp parameter will not used as predictor based on the results above.


## Formulas

```{r }

Formula1 = as.formula(HourlyWage~age)
Formula2 = as.formula(HourlyWage~age+race)
Formula3 = as.formula(HourlyWage~age+race+grade92)
Formula4 = as.formula(HourlyWage~age+race+grade92+sex+class)

```

## Training the model 

```{r }
Model1 = lm(Formula1,df)
Model2 = lm(Formula2,df)
Model3 = lm(Formula3,df)
Model4 = lm(Formula4,df)

```

## RMSE Calculation 

```{r, warning=FALSE}
df$PredictM1 = predict(Model1,df)
RMSE1 = accuracy(df$PredictM1,df$HourlyWage)[2]

df$PredictM2 = predict(Model1,df)
RMSE2 = accuracy(df$PredictM2,df$HourlyWage)[2]

df$PredictM3 = predict(Model3,df)
RMSE3 = accuracy(df$PredictM3,df$HourlyWage)[2]

df$PredictM4 = predict(Model4,df)
RMSE4 = accuracy(df$PredictM4,df$HourlyWage)[2]

```


## BIC results of each model

```{r }
Bic1 = BIC(Model1)
Bic2 = BIC(Model2)
Bic3 = BIC(Model3)
Bic4 = BIC(Model4)

```


```{r }
Results = data.frame(Model = c("Model 1","Model 2","Model 3","Model 4"),
                     RMSE = c(RMSE1,RMSE2,RMSE3,RMSE4),
                     BIC =c(Bic1,Bic2,Bic3,Bic4))

Results
```

Based on RMSE, model4 is best. However, model1 has the lowest BIC so it could be used for predicting the hourly wage.

## Cross-validation

### cross fold validation with 5 folds

```{r }

train.control <- trainControl(method = "cv", number = 5)

```



## Model Training - Cross Validation

```{r, warning=FALSE}
 

Model11 <- train(Formula1, data = df, method = "lm",
               trControl = train.control)

Model22 <- train(Formula2, data = df, method = "lm",
                 trControl = train.control)

Model33 <- train(Formula3, data = df, method = "lm",
                 trControl = train.control)

Model44 <- train(Formula4, data = df, method = "lm",
                 trControl = train.control)

```



```{r }

RMSE = data.frame(Resample = c('Fold 1','Fold 2','Fold 3','Fold 4','Fold 5','Average'),
                  Model1 = c(Model11$resample$RMSE,mean(Model11$resample$RMSE)),
                  Model2 = c(Model22$resample$RMSE,mean(Model22$resample$RMSE)),
                  Model3 = c(Model33$resample$RMSE,mean(Model33$resample$RMSE)),
                  Model4 = c(Model44$resample$RMSE,mean(Model44$resample$RMSE))
                  )
RMSE
```

The cross validation shows that model2 has the lowest RMSE and it should be used for predicting the parameter of interest. 


