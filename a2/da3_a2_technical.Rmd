---
title: "da3-a2-detailed"
author: "Abigail Chen"
prettydoc::html_pretty:
    theme: cayman
    highlight: github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#loading the libraries
library(fixest)
library(ggplot2)
library( tidyverse)
library( dplyr )
library( kableExtra )
library( modelsummary )
library( DataExplorer )
library( lspline )
library( huxtable )
library( ggthemes )
library( estimatr )
library(forecast)
library(tidyverse)
library('caret')
library('SciViews')
library('forecast' )
library(kableExtra)
options(scipen=999)
df <- read.csv("/Users/abigailchristinechen/da3/a2/Data/raw_data.csv")
```

## Research Question

The task is to help a company operating small and mid-size apartments hosting 2-6 guests. The company is set to price their new apartments not on the market and the task is to help them make business decisions. The goal is to help them build a price prediction model.

## Introduction
  
This project is a linear regression analysis on data about Airbnb rentals in Hawaii, USA.  The dataset is taken from **Inside Airbnb Website** (http://insideairbnb.com/get-the-data.html). The relationship between price and room type, neighborhood along with other parameters will be used.  The raw dataset has 24,294 observations.


## Exploratory Data Analysis

First, the structure of the data was analysed.  Then the various data types were changed to categorical from character format. The price had outliers and its distribution was right skewed. The "ln" was used and outliers were also removed from the list.  So, for modeling our dependent variable was ln_price.

### Variables Selection
Below is the list of shortlisted variables and the reasons behind their selection

1. **ln_price**:  This is our dependent variable.  

2. **room_type**: Entire home/apt has highest and shared room has lowest, tried using anova test to show that different room types have differnt affect on ln_price.

3. **neighbourhood_group**: Significant difference in ln_prices were seen based on neighbourhood_group, Honolulu has least and Maui has the highest  

4. **minimum_nights**: The price seems to increase with increase in minimum nights , plot and anova test proves it. 


## Main model Linear Regression : lm

## Formulas for seven lm models

$$lnPrices=\beta_0+\beta_1(roomtype) $$
$$lnPrices=\beta_0+\beta_1(neighbourhoodGroup) $$
$$lnPrices=\beta_0+\beta_1(minimumNights) $$



$$lnPrices=\beta_0+\beta_1(roomtype)+\beta_2(neighbourhoodGroup) $$

$$lnPrices=\beta_0+\beta_1(roomtype)+\beta_2(minimumNights) $$

$$lnPrices=\beta_0+\beta_1(neighbourhoodGroup)+\beta_2(minimumNights) $$


$$lnPrices=\beta_0+\beta_1(minimumNights)+\beta_2(minimumNights)+\beta_3(roomtype) $$


## External Validity

The data set was divided randomly into train and test in 80-20 percent ratio. 


## Alternative model: LASSO

Lasso regression is a classification algorithm that uses shrinkage in simple and sparse models(i.e model with fewer parameters). In Shrinkage, data values are shrunk towards a central point like the mean. Lasso regression is a regularized regression algorithm that performs L1 regularization which adds penalty equal to the absolute value of the magnitude of coefficients.

“LASSO” stands for Least Absolute Shrinkage and Selection Operator. Lasso regression is good for models showing high levels of multicollinearity or when you want to automate certain parts of model selection i.e variable selection or parameter elimination."
so formula ln_price ~. we dont need preprocess and feature selaction. results of the Lasso model were not better but were close to Model 7 .  Additional **source**(https://www.geeksforgeeks.org/lasso-regression-in-r-programming/).



## Conclusion

 Ideally, lower RMSE and higher R-squared values are indicative of a good model.In conclusion, we can say that gender influences Bonus of a person..... rlm didnt show much differnce mix performance good in some bad in others ... 


## **APPENDIX**

# Data Cleaning

## Looking at column names
There are 18 columns initially 

```{r , echo=TRUE}
names(df)
```


## Initial Check:  

Removing the data we don't need. We dont need to look at : "id" , "name" , "host_id" , "host_name", 'longitude', 'latitude','last_review'.

```{r , echo=TRUE}
df = df[,c("price" , "neighbourhood_group" , "neighbourhood" , "minimum_nights",
           'number_of_reviews', 'reviews_per_month','calculated_host_listings_count',
           'availability_365','number_of_reviews_ltm','license','room_type')]
names(df)
```


## Looking at the structure 

```{r , echo=TRUE}
str(df)
```


## Fixing data types
Converting to factor format

```{r , echo=TRUE}

df$neighbourhood_group = as.factor(df$neighbourhood_group)

df$neighbourhood = as.factor(df$neighbourhood )

df$room_type = as.factor(df$room_type )

str(df)

```


## License :
Changing license to binary category, assuming missing ones do not have a license.

```{r , echo=TRUE}

df$license[df$license != ''] = 1

df$license[df$license == ''] = 0

df$license = as.factor(df$license )

summary(df$license)

```


## Summary of the data

```{r , echo=TRUE}
summary(df)
```

There are 4964 NAs in "reviews per month" since the number of missing reviews per month is very large, we will drop them from our data.

```{r , echo=TRUE}
df = na.omit(df)
```

## Analyzing Price 
The median price is \$245 while the median is \$447.3

```{r , echo=TRUE}
summary(df$price)

```
Here we can see a huge difference between 3rd quartile and max value.

### Visualizing the distribution of price:histogram

```{r , echo=TRUE}
ggplot(df, aes(x=price)) + 
  geom_histogram(color="darkcyan", fill="darkcyan",binwidth=100) +
  ggtitle( "Figure 1: Price Distribution" )

```

Checking for outliers in the data.


### Taking ln of price 

```{r , echo=TRUE, error=FALSE,warning=FALSE}

df$ln_price = ln(df$price)

ggplot(df, aes(x=ln_price)) + 
  geom_histogram(color="darkcyan", fill="darkcyan",bins = 30) +
   ggtitle( "Figure 2: ln_price" )
```

ln has turned it into a normal distribution

## removing the outliers
The new number of observations is now 24,197.
```{r , echo=TRUE}

df <- subset(df, ln_price>3 & ln_price<9 )

ggplot(df, aes(x=ln_price)) + 
  geom_histogram(color="darkcyan", fill="darkcyan",bins = 30) +
   ggtitle( "Figure 2: ln_price" )

```

ln_price will be used for analysis instead of price. 

## Analyzing minimum_nights 
The median number of nigths is 3 and the mean is 6.4.  

```{r , echo=TRUE}

summary(df$minimum_nights)

```

### Histogram 

```{r , echo=TRUE}

ggplot(df, aes(x=minimum_nights)) + 
  geom_histogram(color="darkcyan", fill="darkcyan", binwidth = 1) +
  ggtitle( "Figure 3: Minimum Nights Distribution" )

```
Removing observations with minimum nights above 10 nights.

```{r , echo=TRUE}

df1 <- subset(df, minimum_nights<9)

ggplot(df1, aes(x=minimum_nights)) + 
  geom_histogram(color="darkcyan", fill="darkcyan", binwidth = 1) +
  ggtitle( "Figure 3: Minimum Nights Distribution Updated" )
```

```{r , echo=TRUE}
## minimum night looks like a category 
## changing the data type

df1$minimum_nights = as.factor(df1$minimum_nights)

ggplot(df1, aes(x=minimum_nights, y=ln_price, color = minimum_nights)) +
  geom_boxplot() +
  ggtitle("Figure 4 : Minimum Nights")
```
The price seems to increase with increase in minimum nights , plot and anova test shows it.
```{r , echo=TRUE}
ANOVAln_price_minimum_nights <- aov(ln_price ~ minimum_nights, data = df1)

summary(ANOVAln_price_minimum_nights)

df = df1

```

## Analyzing number_of_reviews 
The max reviews is 878, while the mean is 29 reviews. The 3rd quartile is just 33. 
There is a significant differnce in 3rd quartile and max hence outliers must be present.
```{r , echo=TRUE}
summary(df$number_of_reviews)
```

### Histogram
Trying to remove values greater than 150.
The new number of observations is now 20,836.

```{r , echo=TRUE}
ggplot(df, aes(x=number_of_reviews)) + 
  geom_histogram(color="darkcyan", fill="darkcyan", binwidth = 1) +
  ggtitle("Figure 5 : Number of Reviews")

```

```{r , echo=TRUE}
df1 <- subset(df, number_of_reviews<150)
```



```{r , echo=TRUE}
ggplot(df1, aes(x=number_of_reviews)) + 
  geom_histogram(color="darkcyan", fill="darkcyan", binwidth = 1) +
  ggtitle("Figure 6: Number of Reviews")
```


### Scatter Plot: ln_Price vs number_of_reviews
The number_of_reviews don't seem to play any role in determining the ln_price. So it will not be used.

```{r , echo=TRUE}
ggplot(df1, aes(x=number_of_reviews, y=ln_price)) +
  geom_point() +
  ggtitle("Figure 7 :ln_Price vs number_of_reviews ")
```

##Analyzing reviews_per_month 
The Max reviews is 21, while the 3rd quartile is 1.7 reviews. 
This doesn't seem to be related to the ln_price.
```{r , echo=TRUE}
summary(df$reviews_per_month)
```

```{r , echo=TRUE}
ggplot(df, aes(x=reviews_per_month, color = eviews_per_month )) + 
  geom_histogram(color="darkcyan", fill="darkcyan", binwidth = 1) +
  ggtitle("Figure 8 :Reviews per month distribution")

```

```{r , echo=TRUE}

df1 <- subset(df, reviews_per_month < 9)

ggplot(df1, aes(x=reviews_per_month)) + 
  geom_histogram(color="darkcyan", fill="darkcyan", binwidth = 1) +
  ggtitle("Figure 8 :Reviews per month distribution")
```



```{r , echo=TRUE}
ggplot(df1, aes(x=reviews_per_month, y=ln_price)) +
  geom_point() +
  ggtitle("Figure 9 :Reviews per month vs ln_price")
```


## Analyzing calculated_host_listings_count 
The Max listings 439, while the 3rd quartile is 94 and the mean is 67. 
This also doesn't seem play any role in determining the ln_price.
```{r , echo=TRUE}
summary(df$calculated_host_listings_count)
```


```{r , echo=TRUE}

ggplot(df, aes(x=calculated_host_listings_count)) + 
  geom_histogram(color="darkcyan", fill="darkcyan", binwidth = 1)+
  ggtitle("Figure 10: Calculated Host Listings Distribution")
```


```{r , echo=TRUE}
# df1 <- subset(df, calculated_host_listings_count > 1 &calculated_host_listings_count < 7)
df1 <- subset(df, calculated_host_listings_count > 1 &calculated_host_listings_count < 7)

```

```{r , echo=TRUE}
ggplot(df1, aes(x=calculated_host_listings_count)) + 
  geom_histogram(color="darkcyan", fill="darkcyan", binwidth = 1) +
  ggtitle("Figure 10: Calculated Host Listings Distribution")
```

```{r , echo=TRUE}
ggplot(df1, aes(x=calculated_host_listings_count, y=ln_price, color = calculated_host_listings_count)) +
  geom_point() +
  ggtitle("Figure 11: Calculated Host Listings vs ln_price")
```


## Analyzing availability_365 
The median is 213, while the 3rd quartile 278 and the max is 365.
This parameter also doesn't seem to affect the ln_price. 
```{r , echo=TRUE}
summary(df$availability_365)
```


```{r , echo=TRUE}
ggplot(df, aes(x=availability_365, y=ln_price)) +
  geom_point() +
  ggtitle("Figure 12: Availability_365 vs ln_price")
```
## Analyzing number_of_reviews_ltm 
With the summary we can see the 3rd quartile being 15, and the max number at 388.
This shows that there are outliers. There also seems to be no relation with this parameter.
```{r , echo=TRUE}
summary(df$number_of_reviews_ltm)
```


```{r , echo=TRUE}
ggplot(df, aes(x=number_of_reviews_ltm)) + 
  geom_histogram(color="darkcyan", fill="darkcyan", binwidth = 1) +
  ggtitle("Figure 13: number_of_reviews_ltm Distribution ")

```


```{r , echo=TRUE}
df1 <- subset(df, number_of_reviews_ltm < 100)
```


```{r , echo=TRUE}

ggplot(df1, aes(x=number_of_reviews_ltm)) + 
  geom_histogram(color="darkcyan", fill="darkcyan", binwidth = 1) +
  ggtitle("Figure 13: number_of_reviews_ltm Distribution ")
```


```{r , echo=TRUE}
ggplot(df1, aes(x=number_of_reviews_ltm, y=ln_price)) +
  geom_point() +
  ggtitle("Figure 14: number_of_reviews_ltm vs ln_price")
```

## Analyzing neighbourhood_group
There are 4 neighboring groups : Hawaii, Honolulu, Kauai, and Maui.  Maui has the most observations, while Kauai with the least.
There seems to be difference in prices based on  neighbourhood_group: Honolulu has least and Kauai has the highest 
Thers is a significant difference in ln_prices based on neighbourhood_group.
```{r , echo=TRUE}

summary(df$neighbourhood_group)

```

```{r , echo=TRUE}

ggplot(df, aes(x=neighbourhood_group, y=ln_price, color = neighbourhood_group)) +
  geom_boxplot() + 
  ggtitle("Figure 15: neighbourhood_group vs ln_price")
```


```{r , echo=TRUE}

ANOVAln_price_neighbourhood_group <- aov(ln_price ~ neighbourhood_group, data = df)

summary(ANOVAln_price_neighbourhood_group)

```


# Analyzing license 
Checking the relationship between ln_price and license, there is no difference.
```{r , echo=TRUE}
summary(df$license)
```

```{r , echo=TRUE}
ggplot(df, aes(x=license, y=ln_price, color = license)) +
  geom_boxplot() +
  ggtitle("Figure 16: license vs ln_price")
```


## Analyzing room_type
There are 4 types of room: Entire home/apts, Hotel rooms, Private rooms, and Shared rooms. With Entire homes dominating the observations at 19761m and with shared rooms at 31 observations. The Entire home/apt has highest while the shared room has lowest.
The anova test also shows that different room types have different affect on ln_price.eee

```{r , echo=TRUE}
summary(df$room_type)
```


```{r , echo=TRUE}
ggplot(df, aes(x=room_type, y=ln_price, color = room_type)) +
  geom_boxplot() +
 ggtitle("Figure 16: room_type vs ln_price")
```

```{r , echo=TRUE}
ANOVAln_price_room_type <- aov(ln_price ~ room_type, data = df)
summary(ANOVAln_price_room_type)

```

## Pearson Correlation: Numeric independent variables with Price
Correlation is very low between numeric variables and dependent variable. 
```{r , echo=TRUE}

## Get the list of the categorical data

categoricalList = names(df %>% select(where(is.factor)) %>% glimpse())

## getting list of the non categorical data, which will be numeric or int

numericList = names(df)[!names(df) %in% categoricalList] 

cor(df[,numericList])

```


## Working with the Selected Variables 
```{r , echo=TRUE}

df = df[,c('ln_price','room_type','neighbourhood_group','minimum_nights')]

names(df)
```

## Train Test data division 80-20: create a holdout set (20% of observations)

```{r , echo=TRUE}

set.seed(100)
n = nrow(df)
trainIndex = sample(1:n, size = round(0.8*n), replace=FALSE)
train = df[trainIndex ,]
test = df[-trainIndex ,]

```



```{r , echo=TRUE}

Formula1 = as.formula(ln_price~room_type)
Formula2 = as.formula(ln_price~neighbourhood_group)
Formula3 = as.formula(ln_price~minimum_nights)

Formula4 = as.formula(ln_price~room_type+neighbourhood_group)
Formula5 = as.formula(ln_price~room_type+minimum_nights)
Formula6 = as.formula(ln_price~neighbourhood_group+minimum_nights)

Formula7 = as.formula(ln_price~neighbourhood_group+minimum_nights+room_type)

FormulaList = c(Formula1,Formula2,Formula3,Formula4,Formula5,Formula6,Formula7)

```


## Linear modeling : lm

```{r , echo=TRUE}

Results = data.frame()
TestDataAccuracyAlldata = data.frame()
TestDataAccuracyAlldataK = data.frame()

for ( i in 1:7 ){
  
  formula <- as.formula( unlist( FormulaList[[i]] ))
  
  # running the model on entire data set 
  model_work_data <- feols( formula , data = train , vcov='hetero' )

  
  fs  <- fitstat(model_work_data,c('rmse','r2','bic'))
  BIC <- fs$bic
  r2  <- fs$r2
  rmse_train <- fs$rmse
  ncoeff <- length( model_work_data$coefficients )
  
  # Do the k-fold cross validation with 5 folds
  
  cv_i <- train( formula, train, method = "lm", 
                 trControl = trainControl(method = "cv", number = 5))
  
  rmse_k <- mean( cv_i$resample$RMSE )
  
  # Save the results
  TempResults = data.frame(Model = paste("Model",i),
                           Formula = as.character(( FormulaList[i] )),
                           Coefficients=ncoeff,
                           R_squared=r2,
                           BIC = BIC,
                           RMSE_Train = rmse_train,
                           RMSE_K_Fold = rmse_k
                           )
  
  ### External Validity
  
  ## LM Robust Model performance on test data
  
  ## Model performance on test data
  
  test$PredictM1R = predict(model_work_data,test)
  ACCR = accuracy(test$PredictM1R,test$ln_price)
  
  TempTestDataAccuracyAlldata = data.frame(Model = c(paste('Model',i)),
                                         ME = c(ACCR[1]),
                                         RMSE = c(ACCR[2]),
                                         MAE = c(ACCR[3]),
                                         MPE = c(ACCR[4]),
                                         MAPE = c(ACCR[5])
                                         )
  
  test$PredictM1R = predict(cv_i,test)
  ACCR = accuracy(test$PredictM1R,test$ln_price)
  
  TempTestDataAccuracyAlldataK = data.frame(Model = c(paste('Model_K',i)),
                                         ME = c(ACCR[1]),
                                         RMSE = c(ACCR[2]),
                                         MAE = c(ACCR[3]),
                                         MPE = c(ACCR[4]),
                                         MAPE = c(ACCR[5])
                                         )
  
  TestDataAccuracyAlldata = rbind(TestDataAccuracyAlldata ,TempTestDataAccuracyAlldata )
  TestDataAccuracyAlldataK = rbind(TestDataAccuracyAlldataK ,TempTestDataAccuracyAlldataK )
  
  Results = rbind(Results,TempResults)
}



rownames(Results) <- NULL
Results


```


## All data Model Performance on test data

```{r , echo=TRUE}

TestDataAccuracyAlldata


```


## k folds  Model Performance on test data
```{r , echo=TRUE}
TestDataAccuracyAlldataK

write.csv(TestDataAccuracyAlldataK,"TestData.csv",row.names = FALSE)
```


## Alternative model LASSO

```{r , echo=TRUE, warning=FALSE}


# Set lasso tuning parameters:

# a) basic setup
train_control <- trainControl( method = "cv", number = 5)

# b) tell the actual lambda (penalty parameter) to use for lasso
tune_grid     <- expand.grid("alpha" = c(1), "lambda" = seq(0.05, 1, by = 0.05))



# Run LASSO

lasso_model <- train(ln_price ~.,
                            data = train,
                            method = "glmnet",
                            preProcess = c("center", "scale"),
                            trControl = trainControl( method = "cv", number = 5),
                            tuneGrid = tune_grid,
                            na.action=na.exclude)


# One can get the coefficients as well
lasso_coeffs <- coef(lasso_model$finalModel, lasso_model$bestTune$lambda) %>%
  as.matrix() %>%
  as.data.frame() %>%
  rownames_to_column(var = "variable") %>%
  rename(coefficient = `s1`)  # the column has a name "1", to be renamed


# Check the number of variables which actually has coefficients other than 0
lasso_coeffs_nz<-lasso_coeffs %>%
  filter(coefficient!=0)


# Get the RMSE of the Lasso model 
#   Note you should compare this to the test RMSE
lasso_fitstats <- lasso_model$results %>%
  filter(lambda == lasso_model$bestTune$lambda) 
lasso_fitstats



```


### results of lasso model 

```{r , echo=TRUE}

lasso_add <- data.frame(Model='LASSO', Coefficients=nrow(lasso_coeffs_nz),
                    Formula = "ln_price ~.",
                    R_squared=lasso_fitstats$Rsquared, BIC = NA, 
                    RMSE_Train = NA, RMSE_K_Fold = lasso_fitstats$RMSE )

lasso_add

```


### Adding it to previous  results

```{r , echo=TRUE}

Results <- rbind( Results , lasso_add )
rownames(Results) <- NULL
Results
write.csv(Results,"Results.csv",row.names = FALSE)
```




### BIC results

```{r , echo=TRUE, warning=FALSE}
ggplot(Results, aes(x=Model, y=BIC, group=1)) +
  geom_line(color="red",linetype = "dashed")+
  geom_point() +
  ggtitle("Results: Models vs BIC")

```
Model 7 has the lowest BIC so it should be used 


### RMSE_Train results

```{r , echo=TRUE, warning=FALSE}

ggplot(Results, aes(x=Model, y=RMSE_Train, group=1)) +
  geom_line(color="red",linetype = "dashed")+
  geom_point() + 
  ggtitle("Results: Models vs RMSE Train")
```



### RMSE_K_Fold results

```{r , echo=TRUE, warning=FALSE}

ggplot(Results, aes(x=Model, y=RMSE_K_Fold, group=1)) +
  geom_line(color="red",linetype = "dashed")+
  geom_point() +
  ggtitle("Results: Models vs RMSE K fold")
```

Model 7 has the lowest RMSE, less than lasso as well, So BIC and RMSE both suggest Model 7 is best

## Model Performance on test data: RMSE results

```{r , echo=TRUE}

ggplot(TestDataAccuracyAlldata, aes(x=Model, y=RMSE, group=1)) +
  geom_line(color="red",linetype = "dashed")+
  geom_point() +
  ggtitle("Results: Models vs RMSE")

```
Model 7 performs better on test data as well.





